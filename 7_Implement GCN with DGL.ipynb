{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97573179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self,in_feats,out_feats,bias=True):\n",
    "        super(GCNLayer,self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_feats,out_feats))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_feats))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.reset_parameter()\n",
    "        \n",
    "    def reset_parameter(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "    \n",
    "    def forward(self,g,h):\n",
    "        with g.local_scope():\n",
    "            h = torch.matmul(h,self.weight)\n",
    "            g.ndata['h'] = h * g.ndata['norm']\n",
    "            g.update_all(message_func = fn.copy_u('h','m'),\n",
    "                            reduce_func=fn.sum('m','h'))\n",
    "            h = g.ndata['h']\n",
    "            h = h * g.ndata['norm']\n",
    "            if self.bias is not None:\n",
    "                h = h + self.bias\n",
    "            return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d82c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self,in_feats,h_feats,num_classes,bias=True):\n",
    "        super(GCNModel,self).__init__()\n",
    "        self.conv1 = GCNLayer(in_feats,h_feats,bias)\n",
    "        self.conv2 = GCNLayer(h_feats,num_classes,bias)\n",
    "    \n",
    "    def forward(self,g,in_feat):\n",
    "        h = self.conv1(g,in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g,h)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8511e3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch 5, loss: 1.897, train acc: 0.636, val acc: 0.334 (best 0.334), test acc: 0.352 (best 0.352)\n",
      "Epoch 10, loss: 1.800, train acc: 0.821, val acc: 0.530 (best 0.530), test acc: 0.578 (best 0.578)\n",
      "Epoch 15, loss: 1.672, train acc: 0.900, val acc: 0.592 (best 0.592), test acc: 0.610 (best 0.610)\n",
      "Epoch 20, loss: 1.518, train acc: 0.943, val acc: 0.666 (best 0.666), test acc: 0.696 (best 0.696)\n",
      "Epoch 25, loss: 1.348, train acc: 0.943, val acc: 0.722 (best 0.722), test acc: 0.739 (best 0.739)\n",
      "Epoch 30, loss: 1.173, train acc: 0.950, val acc: 0.730 (best 0.730), test acc: 0.760 (best 0.760)\n",
      "Epoch 35, loss: 1.003, train acc: 0.964, val acc: 0.764 (best 0.764), test acc: 0.784 (best 0.784)\n",
      "Epoch 40, loss: 0.851, train acc: 0.971, val acc: 0.770 (best 0.772), test acc: 0.800 (best 0.797)\n",
      "Epoch 45, loss: 0.722, train acc: 0.971, val acc: 0.780 (best 0.780), test acc: 0.806 (best 0.806)\n",
      "Epoch 50, loss: 0.620, train acc: 0.979, val acc: 0.780 (best 0.782), test acc: 0.809 (best 0.812)\n",
      "Epoch 55, loss: 0.541, train acc: 0.986, val acc: 0.782 (best 0.784), test acc: 0.814 (best 0.812)\n",
      "Epoch 60, loss: 0.480, train acc: 0.986, val acc: 0.784 (best 0.784), test acc: 0.814 (best 0.812)\n",
      "Epoch 65, loss: 0.433, train acc: 0.986, val acc: 0.786 (best 0.786), test acc: 0.815 (best 0.814)\n",
      "Epoch 70, loss: 0.396, train acc: 0.986, val acc: 0.788 (best 0.788), test acc: 0.815 (best 0.814)\n",
      "Epoch 75, loss: 0.366, train acc: 0.993, val acc: 0.788 (best 0.788), test acc: 0.816 (best 0.814)\n",
      "Epoch 80, loss: 0.341, train acc: 0.993, val acc: 0.790 (best 0.790), test acc: 0.813 (best 0.813)\n",
      "Epoch 85, loss: 0.320, train acc: 0.993, val acc: 0.788 (best 0.790), test acc: 0.814 (best 0.813)\n",
      "Epoch 90, loss: 0.302, train acc: 1.000, val acc: 0.790 (best 0.790), test acc: 0.814 (best 0.813)\n",
      "Epoch 95, loss: 0.287, train acc: 1.000, val acc: 0.792 (best 0.792), test acc: 0.812 (best 0.812)\n",
      "Epoch 100, loss: 0.274, train acc: 1.000, val acc: 0.790 (best 0.792), test acc: 0.812 (best 0.812)\n",
      "Epoch 105, loss: 0.262, train acc: 1.000, val acc: 0.790 (best 0.792), test acc: 0.811 (best 0.812)\n",
      "Epoch 110, loss: 0.252, train acc: 1.000, val acc: 0.792 (best 0.792), test acc: 0.807 (best 0.812)\n",
      "Epoch 115, loss: 0.243, train acc: 1.000, val acc: 0.790 (best 0.792), test acc: 0.807 (best 0.812)\n",
      "Epoch 120, loss: 0.234, train acc: 1.000, val acc: 0.790 (best 0.792), test acc: 0.810 (best 0.812)\n",
      "Epoch 125, loss: 0.227, train acc: 1.000, val acc: 0.788 (best 0.792), test acc: 0.811 (best 0.812)\n",
      "Epoch 130, loss: 0.220, train acc: 1.000, val acc: 0.786 (best 0.792), test acc: 0.812 (best 0.812)\n",
      "Epoch 135, loss: 0.214, train acc: 1.000, val acc: 0.786 (best 0.792), test acc: 0.813 (best 0.812)\n",
      "Epoch 140, loss: 0.208, train acc: 1.000, val acc: 0.788 (best 0.792), test acc: 0.814 (best 0.812)\n",
      "Epoch 145, loss: 0.203, train acc: 1.000, val acc: 0.788 (best 0.792), test acc: 0.811 (best 0.812)\n",
      "Epoch 150, loss: 0.198, train acc: 1.000, val acc: 0.788 (best 0.792), test acc: 0.811 (best 0.812)\n",
      "Epoch 155, loss: 0.194, train acc: 1.000, val acc: 0.788 (best 0.792), test acc: 0.809 (best 0.812)\n",
      "Epoch 160, loss: 0.190, train acc: 1.000, val acc: 0.790 (best 0.792), test acc: 0.808 (best 0.812)\n",
      "Epoch 165, loss: 0.186, train acc: 1.000, val acc: 0.792 (best 0.792), test acc: 0.808 (best 0.812)\n",
      "Epoch 170, loss: 0.182, train acc: 1.000, val acc: 0.794 (best 0.794), test acc: 0.808 (best 0.808)\n",
      "Epoch 175, loss: 0.179, train acc: 1.000, val acc: 0.792 (best 0.794), test acc: 0.808 (best 0.808)\n",
      "Epoch 180, loss: 0.176, train acc: 1.000, val acc: 0.792 (best 0.794), test acc: 0.808 (best 0.808)\n",
      "Epoch 185, loss: 0.173, train acc: 1.000, val acc: 0.792 (best 0.794), test acc: 0.807 (best 0.808)\n",
      "Epoch 190, loss: 0.170, train acc: 1.000, val acc: 0.792 (best 0.794), test acc: 0.809 (best 0.808)\n",
      "Epoch 195, loss: 0.167, train acc: 1.000, val acc: 0.792 (best 0.794), test acc: 0.809 (best 0.808)\n",
      "Epoch 200, loss: 0.165, train acc: 1.000, val acc: 0.792 (best 0.794), test acc: 0.809 (best 0.808)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def train(g, model, optimizer, loss_fn, epochs):\n",
    "    best_val_acc, best_test_acc = 0, 0\n",
    "    # gain features and labels of datasets\n",
    "    features, labels = g.ndata['feat'], g.ndata['label']\n",
    "    train_mask, val_mask, test_mask = g.ndata['train_mask'], g.ndata[\n",
    "        'val_mask'], g.ndata['test_mask']\n",
    "    train_acc_list, val_acc_list, test_acc_list = [], [], []\n",
    "    for e in range(epochs):\n",
    "        logits = model(g, features)  # (N, label_nums)\n",
    "        pred = logits.argmax(1)\n",
    "        loss = loss_fn(logits[train_mask], labels[train_mask])\n",
    "        # cal trian acc\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        # cal val acc\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        # cal test acc\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        train_acc_list.append(train_acc.item())\n",
    "        val_acc_list.append(val_acc.item())\n",
    "        test_acc_list.append(test_acc.item())\n",
    "\n",
    "        # save result based on valid dataset\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (e + 1) % 5 == 0:\n",
    "            print(\n",
    "                'Epoch {}, loss: {:.3f}, train acc: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'\n",
    "                .format(e + 1, loss, train_acc, val_acc, best_val_acc,\n",
    "                        test_acc, best_test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    epochs = 200\n",
    "    hidden_size = 32\n",
    "    lr = 0.01\n",
    "    weight_decay = 5e-4\n",
    "    # download and loading dataset\n",
    "    dataset = dgl.data.CoraGraphDataset(raw_dir=\"../temp/DGL/\")\n",
    "    g = dataset[0]\n",
    "    # add self-loop\n",
    "    g = dgl.remove_self_loop(g)\n",
    "    g = dgl.add_self_loop(g)\n",
    "    # gain degree matrix\n",
    "    degs = g.out_degrees().float()\n",
    "    # cal D^{-1/2}\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    model = GCNModel(in_feats=g.ndata['feat'].shape[1],\n",
    "                     h_feats=hidden_size,\n",
    "                     num_classes=dataset.num_classes)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        g = g.to(device)\n",
    "        model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train(g, model, optimizer, loss_fn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353b0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNNpt1.13_cpu",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
