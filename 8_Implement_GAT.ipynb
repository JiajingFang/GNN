{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb297f13",
   "metadata": {},
   "source": [
    "<h4>The GAT layer is defined as:</h4>\n",
    "\n",
    "1$$ e_{ij}^{l} = LeakyReLU(a(Wh_{i}^{(l)} || Wh_{j}^{(l)}))$$ \n",
    "2$$ a_{ij}^{l} = softmax_{j}(e_{ij}^{l})= \\dfrac{\\exp(e_{ij}^{l})}{\\sum_{k \\in N(i)} \\exp(e_{ik}^{l})}$$ \n",
    "3$$h_{i}^{(l+1)} = \\sum_{j \\in N(i)} a_{ij} W^{(l)} h_{j}^{(l)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9534aa",
   "metadata": {},
   "source": [
    "<h4>add Multi-Head Attention Mechanism, the last aggregate equation can be rewritten as follows:</h4> the dimensions is num_head $*$ hidden layer dimensions\n",
    "\n",
    "$$h_{i}^{l+1} = ||_{k=1}^{K} \\sigma (\\sum_{j \\in N(i)} a_{ij}^{k} W^{k} h_{j}^{l})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5d4ec",
   "metadata": {},
   "source": [
    "<h4>the last layer uses average instead of cat</h4>\n",
    "\n",
    "$$h_{i}^{l+1} = \\sigma (\\dfrac{1}{K}\\sum_{k=1}^{K}\\sum_{j \\in N(i)} a_{ij}^{k} W^{k} h_{j}^{l})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e4de656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import softmax, add_remaining_self_loops\n",
    "\n",
    "class GATConv(MessagePassing):\n",
    "    def __init__(self, in_feats, out_feats, alpha, drop_prob, num_heads):\n",
    "        super().__init__(aggr=\"add\")\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_heads = num_heads\n",
    "        self.out_feats = out_feats // num_heads\n",
    "        self.lin = nn.Linear(in_feats, self.out_feats*self.num_heads, bias=False)\n",
    "        self.a = nn.Linear(2*self.out_feats, 1)\n",
    "        self.leakrelu = nn.LeakyReLU(alpha)\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, _ = add_remaining_self_loops(edge_index)\n",
    "#         W H\n",
    "        h = self.lin(x)\n",
    "        h_p = self.propagate(edge_index, x=h)\n",
    "        return h_p\n",
    "    def message(self, x_i, x_j, edge_index_i):\n",
    "        x_i = x_i.view(-1, self.num_heads, self.out_feats)\n",
    "        x_j = x_j.view(-1, self.num_heads, self.out_feats)\n",
    "#         a(wh_i || wh_j)\n",
    "        e = self.a(torch.cat([x_i, x_j], dim=-1))\n",
    "#         LeakReLU(a(Wh_i, Wh_j))\n",
    "\n",
    "        e = self.leakrelu(e)\n",
    "#         softmax(e_{ij})\n",
    "\n",
    "        alpha = softmax(e, edge_index_i)\n",
    "\n",
    "        alpha = F.dropout(alpha, self.drop_prob, self.training)\n",
    "        return (x_j * alpha).view(x_j.size(0), -1)\n",
    "\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, y_num, alpha=0.2, drop_prob=0., num_heads=[1,1]):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.gatconv1 = GATConv(in_feats, hidden_feats, alpha, drop_prob, num_heads[0])\n",
    "        \n",
    "        self.gatconv2 = GATConv(hidden_feats, y_num, alpha, drop_prob, num_heads[1])\n",
    "        \n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gatconv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, self.drop_prob, self.training)\n",
    "        out = self.gatconv2(x, edge_index)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "289fc455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing with a sample dataset\n",
    "conv = GAT(in_feats=8,\n",
    "            hidden_feats=64,\n",
    "            y_num=4,\n",
    "            drop_prob=0.2,\n",
    "            num_heads=[8, 1])\n",
    "x = torch.rand(4, 8)\n",
    "edge_index = torch.tensor(\n",
    "    [[0, 1, 1, 2, 0, 2, 0, 3], [1, 0, 2, 1, 2, 0, 3, 0]], dtype=torch.long)\n",
    "x1 = conv(x, edge_index)\n",
    "print(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84ab764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply in a big sample dataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f32ebfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: train_loss: 0.43038774\n",
      "Epoch 50: train_loss: 0.35651967\n",
      "Epoch 75: train_loss: 0.29772645\n",
      "Epoch 100: train_loss: 0.36352384\n",
      "Epoch 125: train_loss: 0.28289831\n",
      "Epoch 150: train_loss: 0.33076385\n",
      "Epoch 175: train_loss: 0.30606464\n",
      "Epoch 200: train_loss: 0.41505975\n",
      "Epoch 225: train_loss: 0.38383773\n",
      "Epoch 250: train_loss: 0.32141551\n",
      "Epoch 275: train_loss: 0.32326403\n",
      "Epoch 300: train_loss: 0.37980521\n",
      "testset accuracy:0.8120\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset = Planetoid(root='temp/', name='Cora')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = dataset[0].to(device)\n",
    "h = data.x\n",
    "feature_dim = h.shape[1]\n",
    "class_num = dataset.num_classes\n",
    "\n",
    "# define model\n",
    "model = GAT(in_feats=feature_dim,\n",
    "            hidden_feats=64,\n",
    "            y_num=class_num,\n",
    "            drop_prob=0.6,\n",
    "            num_heads=[8,1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "best_acc, best_model = 0. , None\n",
    "\n",
    "# training\n",
    "model.train()\n",
    "for epoch in range(300):\n",
    "    out = model(h, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    valid_acc = accuracy_score(data.y[data.val_mask].cpu(),\n",
    "                               out[data.val_mask].argmax(dim=1).cpu())\n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        best_model = deepcopy(model)\n",
    "    if (epoch+1) % 25 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: train_loss: {loss.item():.8f}\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# evaluation\n",
    "best_model.eval()\n",
    "pred = best_model(h, data.edge_index)\n",
    "test_acc = accuracy_score(data.y[data.test_mask].cpu(),\n",
    "                          pred[data.test_mask].argmax(dim=1).cpu())\n",
    "print(f'testset accuracy:{test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8028b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8df27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNNpt1.13_cpu",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
